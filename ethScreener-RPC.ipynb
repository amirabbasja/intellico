{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check blocks in provided range and find new tokens being initiated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv \n",
    "import json\n",
    "from utils import *\n",
    "from web3 import Web3\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from utils import *\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Get the config file\n",
    "configObj = ConfigManager(\"config.json\")\n",
    "appInfo, configData = configObj.load_config()\n",
    "nodeUrl = appInfo[\"alchemy_url\"]+appInfo[\"alchemy_key\"]\n",
    "\n",
    "# Checking the database state\n",
    "print(\"Checking database integrity ...\")\n",
    "db = dbUtils(user = \"postgres\", password = \"1234\", host  = \"localhost\", port =  \"5432\")\n",
    "# See if database exists\n",
    "if db.database_exists(\"screenerDB\"):\n",
    "    if not db.table_exists(\"screenerDB\", \"tokens\"):\n",
    "        print(\"Creating table 'tokens' in the database 'screenerDB'\")\n",
    "        # Cursor\n",
    "        db._connect_to_db(\"screenerDB\")\n",
    "        con = db._conn\n",
    "        cur = con.cursor()\n",
    "        \n",
    "        # Make a table for the new tokens\n",
    "        cur.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS tokens (\n",
    "                id SERIAL PRIMARY KEY,\n",
    "                address VARCHAR(255),\n",
    "                name VARCHAR(255),\n",
    "                symbol VARCHAR(255),\n",
    "                chain_name VARCHAR(255),\n",
    "                decimals INT,\n",
    "                inception_time BIGINT,\n",
    "                inception_block BIGINT,\n",
    "                total_supply VARCHAR(255)\n",
    "            )\n",
    "        \"\"\")\n",
    "        \n",
    "        con.commit()\n",
    "        \n",
    "        # close the connection\n",
    "        con.close()\n",
    "    else:\n",
    "        pass\n",
    "else:\n",
    "    raise Exception(f\"Database doesn't exist. First create the database with the name 'screenerDB'\")\n",
    "\n",
    "# Make the ETH blockchain handler\n",
    "web3 = Web3(Web3.HTTPProvider(nodeUrl))\n",
    "handler = ETH_Handler(web3)\n",
    "\n",
    "latest_block = handler.get_latest_block()\n",
    "\n",
    "# True only if the config file is new.\n",
    "if (configData['latest_block_checked'] == -1):\n",
    "    configData['latest_block_checked'] = latest_block\n",
    "\n",
    "print(f\"{latest_block['number'] - configData['latest_block_checked']} blocks to check\")\n",
    "\n",
    "nonTokens = pd.DataFrame(columns=['address'])\n",
    "\n",
    "try:\n",
    "    # Go through past blocks to find conteract creation events\n",
    "    for i in tqdm(range(configData['latest_block_checked'], latest_block['number']), total=latest_block['number'] - configData['latest_block_checked']):\n",
    "        block = web3.eth.get_block(i, True)\n",
    "        for tx in block.transactions:\n",
    "            if tx[\"to\"] == None:\n",
    "                tx_receipt = web3.eth.get_transaction_receipt(tx['hash'])\n",
    "                contract_address = tx_receipt['contractAddress']\n",
    "                contract_code = web3.eth.get_code(contract_address)\n",
    "                if contract_code != '0x':\n",
    "                    _details = handler.get_token_details(contract_address)\n",
    "\n",
    "                    if _details != None:\n",
    "                        if _details[\"name\"] != \"-\":\n",
    "                            _data = {\n",
    "                                'address': _details[\"address\"],\n",
    "                                'name': _details[\"name\"],\n",
    "                                'symbol': _details[\"symbol\"],\n",
    "                                'chain_name': \"Ethereum\",\n",
    "                                'decimals': _details[\"decimals\"],\n",
    "                                'inception_time': datetime.now().timestamp(),\n",
    "                                'inception_block': i,\n",
    "                                'total_supply': str(_details[\"total_supply\"])\n",
    "                            }\n",
    "                            state, _ = db.insert_row(appInfo[\"database_name\"], \"tokens\", _data)\n",
    "                            \n",
    "                            # Raise an error if couldn't add to the database \n",
    "                            if not state:\n",
    "                                raise Exception(f\"Error in inserting token {contract_address} in the database\")\n",
    "                        else:\n",
    "                            # DELETE\n",
    "                            nonTokens = pd.concat([nonTokens, pd.DataFrame([contract_address], columns=['address'])])\n",
    "                            pass\n",
    "        configData['latest_block_checked'] = i\n",
    "        configObj.save_config(configData)\n",
    "        nonTokens.to_csv(\"nonTokens.csv\") # DELETE\n",
    "\n",
    "        time.sleep(0.02)\n",
    "    \n",
    "    # Update the latest block checked\n",
    "    configData['latest_block_checked'] = i\n",
    "    configObj.save_config(configData)\n",
    "    print(\"Database updated successfully\")\n",
    "    \n",
    "    # DELETE\n",
    "    nonTokens.to_csv(\"nonTokens.csv\")\n",
    "    \n",
    "except Exception as e:\n",
    "    configData['latest_block_checked'] =  i\n",
    "    configObj.save_config(configData)\n",
    "    print(f\"Error in block {i}\")\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download timestamp of blocks on ethereum and save them to a csv file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, csv\n",
    "from dotenv import load_dotenv \n",
    "import json\n",
    "from utils import *\n",
    "from web3 import Web3\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from utils import *\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the config file\n",
    "configObj = ConfigManager(\"config.json\")\n",
    "appInfo, configData = configObj.load_config()\n",
    "nodeUrl = appInfo[\"alchemy_url\"]+appInfo[\"alchemy_key\"]\n",
    "\n",
    "def __getBlockTimestamp(blockNumber:int, web3Obj:Web3):\n",
    "    \"\"\"\n",
    "    Gets an ethereum block and returns its timestamp and datetime as a dictionary \n",
    "    \"\"\"\n",
    "    __block = web3Obj.eth.get_block(blockNumber, False)\n",
    "    return {\n",
    "        \"block\": blockNumber, \n",
    "        \"timestamp\": __block[\"timestamp\"], \n",
    "        \"datetime\": datetime.fromtimestamp(__block[\"timestamp\"])\n",
    "        }\n",
    "    \n",
    "\n",
    "def __getFiles(path:str):\n",
    "    \"\"\"\n",
    "    Gets the CSV files in a folder\n",
    "    \n",
    "    Returns:\n",
    "        A list of lists [[start block - end block]]\n",
    "        A dictionary with smallest block number and its respective file name\n",
    "        A dictionary with oldest block number and its respective file name\n",
    "    \"\"\"\n",
    "    files = os.listdir(path)\n",
    "    result = []\n",
    "    __maxNumber = 0\n",
    "    __maxNumber_file = \"\"\n",
    "    __minNumber = 99999999999\n",
    "    __minNumber_file = \"\"\n",
    "    \n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            parts = file.split('-')\n",
    "            if len(parts) == 2:\n",
    "                x = int(parts[0])\n",
    "                m = int(parts[1].replace('.csv', ''))\n",
    "                result.append([x, m])\n",
    "                \n",
    "                if __maxNumber < m: __maxNumber = m; __maxNumber_file = file\n",
    "                if x < __minNumber: __minNumber = x; __minNumber_file = file\n",
    "    \n",
    "    return result, {\"block_number\":__minNumber,\"file\":__minNumber_file},{\"block_number\":__maxNumber,\"file\":__maxNumber_file}\n",
    "\n",
    "def __makeCSV(path,startBlockNumber,endBlockNumber,web3, reverse = False):\n",
    "    \"\"\"\n",
    "    Makes a fresh csv file and add blocks to it.\n",
    "    \n",
    "    Returns:\n",
    "        Newly made file's name.\n",
    "    \"\"\"\n",
    "    data = [\n",
    "        ['block', 'timestamp', 'datetime'],\n",
    "    ]\n",
    "    if startBlockNumber < endBlockNumber:\n",
    "        for i in range(startBlockNumber,endBlockNumber+1):\n",
    "            __tmp = __getBlockTimestamp(i,web3)\n",
    "            data.append([__tmp[\"block\"], __tmp[\"timestamp\"], __tmp[\"datetime\"]])\n",
    "    \n",
    "    \n",
    "    if reverse:\n",
    "        data.reverse()\n",
    "        data.insert(0, data.pop())\n",
    "        \n",
    "    with open(os.path.join(path,f\"{startBlockNumber}-{endBlockNumber}.csv\"), 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(data)\n",
    "        f.close()\n",
    "        return f\"{startBlockNumber}-{endBlockNumber}.csv\"\n",
    "\n",
    "def getBlockTimestamps(path:str, direction:str, web3Obj:Web3):\n",
    "    \"\"\"\n",
    "    Gets block timestamps ands saves them in a csv file in passed path.\n",
    "    Each CSV file contains ethereum block numbers, timestamps and datetimes. Blocks \n",
    "    get older as we move down the csv file.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Defining the super parameters\n",
    "    __Default_Start_Block = 21500000 # The first file's starting block (If there are no files in path)\n",
    "    __Batch_length = 100000 # The amount of blocks each csv file can contain\n",
    "    __Target_File = \"\" # The file which newly downloaded blocks are gonna be added to\n",
    "    __Save_Batch_Length = 4000 # Save blocks in the csv file batches of this length\n",
    "    __STOP = False # Weather to stop the downloading process\n",
    "    __STOP_Sub_Batch = False # For checking i with latest block number to see if we have reached the most recent block\n",
    "    \n",
    "    # Get previously saved csv files\n",
    "    if os.path.exists(path):\n",
    "        \n",
    "        # Get previous files\n",
    "        lstFiles = __getFiles(path)\n",
    "        \n",
    "        if len(lstFiles[0]) != 0:\n",
    "            __n1 = lstFiles[1]['block_number']\n",
    "            __n2 = lstFiles[2]['block_number']\n",
    "            print(f\"Found {len(lstFiles[0])} records. Oldest block: {__n1}, most recent block: {__n2}\")\n",
    "        else:\n",
    "            print(f\"No files exist in the directory, making teh first file. Starting from block {__Default_Start_Block}\")\n",
    "            __makeCSV(path,__Default_Start_Block,__Default_Start_Block+1,web3Obj)\n",
    "            __n1 = __Default_Start_Block\n",
    "            __n2 = __Default_Start_Block+1\n",
    "            lstFiles = __getFiles(path)\n",
    "                \n",
    "        # Set the starting block parameter\n",
    "        if direction == \"forward\":\n",
    "            \n",
    "            # Get the target csv file to save the block data\n",
    "            if lstFiles[2][\"block_number\"] % __Batch_length != __Batch_length - 1:\n",
    "                __Target_File = os.path.join(path, lstFiles[2][\"file\"])\n",
    "                __tmp = lstFiles[2][\"block_number\"]\n",
    "                print(f\"Catching up with latest block. Approx, {web3Obj.eth.block_number - __tmp} blocks to chatch up with ...\")\n",
    "            else:\n",
    "                # Happens when the most recent file is full\n",
    "                print(f\"Most recent file is full. Making a new file for blocks bigger than {__n2}\")\n",
    "                __name = __makeCSV(path,lstFiles[2][\"block_number\"]+1,lstFiles[2][\"block_number\"]+2,web3Obj)\n",
    "                __Target_File = os.path.join(path, __name)\n",
    "            \n",
    "            # First and Last block to catch\n",
    "            __Download_Start_Block = int(os.path.basename(__Target_File).replace(\".csv\",\"\").split(\"-\")[1])+1\n",
    "            __Download_End_Block = int(os.path.basename(__Target_File).replace(\".csv\",\"\").split(\"-\")[0]) + __Batch_length - 1\n",
    "            \n",
    "            while not __STOP:\n",
    "                print(f\"Downloading blocks {__Download_Start_Block} to {__Download_End_Block}\")\n",
    "                for batch in range(__Download_Start_Block,__Download_End_Block+1,__Save_Batch_Length):    \n",
    "                    # Open the target file\n",
    "                    with open(__Target_File, 'a', newline='') as f:\n",
    "                        __data = []\n",
    "                        writer = csv.writer(f)\n",
    "                        \n",
    "                        # See if the current sub-batch range has the latest block in it\n",
    "                        if batch <= web3Obj.eth.block_number and web3Obj.eth.block_number <= __Download_End_Block:\n",
    "                            __STOP = True\n",
    "                            __STOP_Sub_Batch = True\n",
    "                        \n",
    "                        for i in tqdm(range(batch, min(batch + __Save_Batch_Length, __Download_End_Block+1))):\n",
    "                            __tmp = __getBlockTimestamp(i,web3)\n",
    "                            __data.append([__tmp[\"block\"], __tmp[\"timestamp\"], __tmp[\"datetime\"]])\n",
    "                            \n",
    "                            # Is true only if the current sub-range range(batch, min(batch + __Save_Batch_Length, __Download_End_Block+1))\n",
    "                            # contains the latest block number\n",
    "                            if __STOP_Sub_Batch:\n",
    "                                if i == web3Obj.eth.block_number: \n",
    "                                    print(f\"Caught up with latest block {i}\")\n",
    "                                    break\n",
    "                        \n",
    "                        # Write the appended data to the file and close it. Then rename it.\n",
    "                        writer.writerows(__data)\n",
    "                        f.close()\n",
    "                        \n",
    "                        # Rename the file after saving new data in it\n",
    "                        os.rename(__Target_File, __Target_File.replace(os.path.basename(__Target_File).replace(\".csv\",\"\").split(\"-\")[1], str(i)))\n",
    "                        \n",
    "                        # Change the parameter for opening the target file in the next iteration\n",
    "                        __Target_File = __Target_File.replace(os.path.basename(__Target_File).replace(\".csv\",\"\").split(\"-\")[1], str(i))\n",
    "\n",
    "                # Batch finished. Make a new file for next batch.\n",
    "                __Target_File = os.path.join(path, __makeCSV(path,i + 1, i + 2,web3Obj))\n",
    "                __Download_Start_Block = i + 1\n",
    "                __Download_End_Block = __Download_Start_Block + __Batch_length - 1\n",
    "                \n",
    "                time.sleep(1)\n",
    "                \n",
    "        elif direction == \"backward\":\n",
    "            \n",
    "            # Get the target csv file to save the block data\n",
    "            if lstFiles[1][\"block_number\"] % __Batch_length != 0:\n",
    "                __Target_File = os.path.join(path, lstFiles[1][\"file\"])\n",
    "            else:\n",
    "                # Happens when the most recent file is full\n",
    "                print(f\"Most recent file is full. Making a new file for blocks smaller than {__n1}\")\n",
    "                __name = __makeCSV(path,lstFiles[1][\"block_number\"]-2,lstFiles[1][\"block_number\"]-1,web3Obj,True)\n",
    "                __Target_File = os.path.join(path, __name)\n",
    "            \n",
    "            # First and Last block to catch\n",
    "            __Download_Start_Block = int(os.path.basename(__Target_File).replace(\".csv\",\"\").split(\"-\")[1]) - __Batch_length + 1\n",
    "            __Download_End_Block = int(os.path.basename(__Target_File).replace(\".csv\",\"\").split(\"-\")[0])\n",
    "            \n",
    "            \n",
    "            while not __STOP:\n",
    "                print(f\"Downloading blocks {__Download_Start_Block} to {__Download_End_Block}\")\n",
    "                for batch in range(__Download_End_Block, __Download_Start_Block-1, - __Save_Batch_Length):     \n",
    "                    # Open the target file\n",
    "                    with open(__Target_File, 'a', newline='') as f:\n",
    "                        __data = []\n",
    "                        writer = csv.writer(f)\n",
    "                        \n",
    "                        for i in tqdm(range(batch - 1, max(batch - __Save_Batch_Length, __Download_Start_Block-1), -1)):\n",
    "                            __tmp = __getBlockTimestamp(i,web3)\n",
    "                            __data.append([__tmp[\"block\"], __tmp[\"timestamp\"], __tmp[\"datetime\"]])\n",
    "                        \n",
    "                        # Write the appended data to the file and close it. Then rename it.\n",
    "                        writer.writerows(__data)\n",
    "                        f.close()\n",
    "                        \n",
    "                        # Rename the file after saving new data in it\n",
    "                        os.rename(__Target_File, __Target_File.replace(os.path.basename(__Target_File).replace(\".csv\",\"\").split(\"-\")[0], str(i)))\n",
    "                        \n",
    "                        # Change the parameter for opening the target file in the next iteration\n",
    "                        __Target_File = __Target_File.replace(os.path.basename(__Target_File).replace(\".csv\",\"\").split(\"-\")[0], str(i))\n",
    "\n",
    "                # Batch finished. Make a new file for next batch.\n",
    "                __Target_File = os.path.join(path, __makeCSV(path, i - 2, i - 1,web3Obj,True))\n",
    "                __Download_Start_Block = i - __Batch_length\n",
    "                __Download_End_Block = i - 2\n",
    "                \n",
    "                time.sleep(1)\n",
    "            \n",
    "        \n",
    "    else:\n",
    "        raise Exception(f\"Path: {path}  Does not exist.\")\n",
    "\n",
    "    \n",
    "# Make the ETH blockchain handler\n",
    "web3 = Web3(Web3.HTTPProvider(nodeUrl))\n",
    "handler = ETH_Handler(web3)\n",
    "# __getFiles(\"./resources\")\n",
    "getBlockTimestamps(\"./resources\", \"backward\", web3Obj = web3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot candlestick chart of a DEX pool "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from utils import *\n",
    "\n",
    "# Get the config file\n",
    "configObj = ConfigManager(\"config.json\")\n",
    "appInfo, configData = configObj.load_config()\n",
    "nodeUrl = appInfo[\"alchemy_url\"]+appInfo[\"alchemy_key\"]\n",
    "\n",
    "# Replace these values with your specific pair\n",
    "PAIR_ADDRESS = \"0xc3db44adc1fcdfd5671f555236eae49f4a8eea18\"; \n",
    "RPC_URL = nodeUrl\n",
    "web3Obj = Web3(Web3.HTTPProvider(nodeUrl))\n",
    "\n",
    "# Get transactions from the last 1000 blocks\n",
    "from_block = 21200000 - 60000\n",
    "end_block = 21300000 + 80000\n",
    "\n",
    "# end_block = web3Obj.eth.block_number\n",
    "# from_block = end_block - 40000\n",
    "\n",
    "handler = ETH_Handler(web3Obj)\n",
    "pairInfo = handler.get_pair_info(PAIR_ADDRESS)\n",
    "quoteAssetIndex = 0 if pairInfo[\"token0\"][\"symbol\"] == \"WETH\" else 1 if pairInfo[\"token1\"][\"symbol\"] == \"WETH\" else None\n",
    "quoteAsset = pairInfo[f\"token{quoteAssetIndex}\"][\"symbol\"]\n",
    "baseAsset = pairInfo[f\"token{quoteAssetIndex-1 if quoteAssetIndex == 1 else 1}\"][\"symbol\"]\n",
    "assetName = f\"{baseAsset}/{quoteAsset}\"\n",
    "\n",
    "candleHandler = tokenCandlestick(\"./resources/ETH_block_data\", web3Obj, \"ETH\", True)\n",
    "\n",
    "transactions = candleHandler.get_uniswap_pair_transactions(\n",
    "    PAIR_ADDRESS, \n",
    "    from_block, \n",
    "    end_block, \n",
    "    None, \n",
    "    pairInfo[\"token0\"][\"decimals\"], \n",
    "    pairInfo[\"token1\"][\"decimals\"], \n",
    "    2 if pairInfo[\"version\"]==\"UNI_V2\" else 3 if pairInfo[\"version\"]==\"UNI_V3\" else -1,\n",
    "    quoteAssetIndex,\n",
    "    0.0001)\n",
    "priceDf = candleHandler.process_transactions(transactions)\n",
    "candleHandler.get_plot(priceDf, \"4h\", {\"title\":assetName})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work with database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the database\n",
    "db = dbUtils(user = \"postgres\", password = \"1234\", host  = \"localhost\", port =  \"5432\")\n",
    "\n",
    "# See if database exists\n",
    "if db.database_exists(\"screenerDB\"):\n",
    "    if not db.table_exists(\"screenerDB\", \"tokens\"):\n",
    "        print(\"Creating table 'tokens' in the database 'screenerDB'\")\n",
    "        # Cursor\n",
    "        db._connect_to_db(\"screenerDB\")\n",
    "        con = db._conn\n",
    "        cur = con.cursor()\n",
    "        \n",
    "        # Make a table for the new tokens\n",
    "        cur.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS tokens (\n",
    "                id SERIAL PRIMARY KEY,\n",
    "                address VARCHAR(255),\n",
    "                name VARCHAR(255),\n",
    "                symbol VARCHAR(255),\n",
    "                chain_name VARCHAR(255),\n",
    "                decimals INT,\n",
    "                inception_time BIGINT,\n",
    "                inception_block BIGINT,\n",
    "                total_supply VARCHAR(255)\n",
    "            )\n",
    "        \"\"\")\n",
    "        \n",
    "        con.commit()\n",
    "        \n",
    "        # close the connection\n",
    "        con.close()\n",
    "    else:\n",
    "        pass\n",
    "else:\n",
    "    print(f\"Database doesn't exist. First create the database with the name 'screenerDB'\")\n",
    "\n",
    "\n",
    "\n",
    "# Insert row\n",
    "from datetime import datetime\n",
    "from utils import dbUtils\n",
    "db = dbUtils(user = \"postgres\", password = \"1234\", host  = \"localhost\", port =  \"5432\")\n",
    "data = {\n",
    "    'address': \"0x248A791B9b3E0e17641A5D0E306B8485403432a9\",\n",
    "    'name': \"PopKitty\",\n",
    "    'symbol': \"POPKI\",\n",
    "    'chain_name': \"Ethereum\",\n",
    "    'decimals': 9,\n",
    "    'inception_time': datetime.now().timestamp(),\n",
    "    'inception_block': 2155211421,\n",
    "    'total_supply': str(100000000000000000000000000)\n",
    "}\n",
    "db.insert_row(\"screenerDB\", \"tokens\", data)\n",
    "\n",
    "# Delete row\n",
    "success, count = db.delete_row(appInfo[\"database_name\"], \"tokens\", \"id = %s\", 3 )\n",
    "\n",
    "# Update row\n",
    "data[\"address\"] = \"0x111\"\n",
    "db.update_row(\n",
    "    appInfo[\"database_name\"],\n",
    "    \"tokens\",\n",
    "    data,\n",
    "    \"id = %s\",\n",
    "    1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get transactions form solana pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime\n",
    "import time\n",
    "import base58\n",
    "import pandas as pd\n",
    "import json\n",
    "from time import sleep\n",
    "from pprint import pprint\n",
    "from utils import SOL_Handler\n",
    "solHandler = SOL_Handler(\"gMrbyp_Ydp55S-AfZPNLztwYS_pa1Z59\")\n",
    "\n",
    "POOL_ADDRESS = \"4RXS3X9ZkrzXkdM1d7MLFwEGHUkYHHG5o353525qyUqM\"\n",
    "\n",
    "# Fetch trades\n",
    "trades = solHandler.getPoolTrades(POOL_ADDRESS, limit = 20, verbose = True, \n",
    "                            searchEnd = {\n",
    "                                \"slot\":315063368,\n",
    "                                \"timestamp\":int(time.time())-5*60\n",
    "                                })\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get OHLCV data for a coin using coingecko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting a new batch from 1738074275 with 1000 candles.\n",
      "Getting a new batch from 1737936000 with 1000 candles.\n",
      "(2, 5)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from utils import coinGeckoCandles\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "df = coinGeckoCandles(\"B1ciFBa9LLZsZcpVjHN7jr7YtV39HFUwJV1UaFYSeiV7\", \"solana\", \"1d\", \n",
    "                startDate=datetime.now(), endDate=datetime.now()-timedelta(days=200),\n",
    "                limit=1000, plotDetails={\"plot\":True, \"type\":\"mplfinance\", \"savefig\":\"./\"}, verbose=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aaa/vk.txt'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path.join(\"aaa/\",\"vk.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create an empty DataFrame with MultiIndex\n",
    "index = pd.MultiIndex(levels=[[], []], codes=[[], []], names=['Asset', 'Date'])\n",
    "columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "_df = pd.DataFrame(columns=columns, index=index)\n",
    "\n",
    "def add_asset_data(df, asset_name, dates, ohlcv_data):\n",
    "    \"\"\"\n",
    "    Add OHLCV data for a new asset to the DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The existing DataFrame.\n",
    "        asset_name (str): Name of the asset (e.g., 'Asset1').\n",
    "        dates (pd.DatetimeIndex): Dates for the OHLCV data.\n",
    "        ohlcv_data (np.ndarray): OHLCV data as a 2D array (rows: dates, columns: OHLCV fields).\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Updated DataFrame with the new asset data.\n",
    "    \"\"\"\n",
    "    # Create a temporary DataFrame for the new asset\n",
    "    temp_df = pd.DataFrame(\n",
    "        ohlcv_data,\n",
    "        index=pd.MultiIndex.from_product([[asset_name], dates], names=['Asset', 'Date']),\n",
    "        columns=columns\n",
    "    )\n",
    "    \n",
    "    # Append the new data to the existing DataFrame\n",
    "    df = pd.concat([df, temp_df])\n",
    "    return df\n",
    "\n",
    "_df = add_asset_data(_df, \"PX\", df.index, df.to_numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".MotherVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
